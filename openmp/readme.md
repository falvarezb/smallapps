# About

Application to illustrate how to use [OpenMP](https://en.wikipedia.org/wiki/OpenMP), an API for shared-memory multiprocessing programming.

A great feature of OpenMP is that the same code can be compiled to run sequentially or in parallel. So a typical approach is to implement a sequential version first and then add OpenMP pragmas to instruct the compiler to run the application in parallel.

The examples on this repo are taken from these [OpenMP resources](https://www.openmp.org/resources/tutorials-articles/)


# Environment configuration (macOS)

Apple ships its own version of [LLVM](https://llvm.org/) with Xcode and does not support OpenMP. Therefore, it is necessary to install LLVM separately:

```
brew install llvm
```

Plus OpenMP library:

```
brew install libomp
```

Then, to build a __'hello world'__:

```
/usr/local/opt/llvm/bin/clang -fopenmp helloworld.c
```

In case it is needed:

```
CPPFLAGS="-I/usr/local/opt/libomp/include"
LDFLAGS="-L/usr/local/opt/libomp/lib"
```

# Parallel programming models with OpenMP

In order to illustrate the different techniques to do parallel programming with OpenMP, we will write a program to approximate the value of Ï€ by doing the numerical integration of:

$$\int_{0}^{1}\frac{4.0}{1+x^2}dx$$

 
Here's different versions of the program:

__pi.c__ --> sequential version decorated with OpenMP's pragmas to illustrate how easy a sequential program can be parallelized without modifying the actual code.

__dandc.c__ --> solution based on the _divide and conquer_ approach that, similarly to _pi.c_, can be compiled to run sequentially or in parallel.

__spmd.c__ --> parallel version based on the SPMD (Single Program Multiple Data) model; this is a naive implementation as it causes [false sharing](https://en.wikipedia.org/wiki/False_sharing) of the array used to store each thread's partial result.

__spmd_padded__ --> improved version of the SPMD program to avoid false sharing by [aligning the shared data to cache line boundaries](http://www.catb.org/esr/structure-packing/).

__spmd_synced.c__ --> another improved version of the SPMD program: instead of having an array to store each thread's partial result, all threads write to a single scalar variable whose access must be synchronized. 


If _pi.c_ is compiled with the flag `-fopenmp`, the compiler will use the pragma directives to make the code parallelizable, otherwise the pragmas will be ignored and the program will be single-threaded. The parallel execution model used here is called _worksharing_ as different processors execute different pathways of the code (in this case different iterations of the loop).

The _divide and conquer version_ is also a _worksharing_ algorithm as the work is split in tasks that are run by different threads. The number of tasks created is controlled by the parallelism factor (new tasks are created as long as the work to do exceeds a pre-defined threshold).

The other versions follow the SPMD model whereby the same pathway is executed by multiple processors, with each processor working on a different subset of data. In order to store the partial result generated by each thread, we need an array indexed by thread id. And since the array elements are contiguous in memory, they are likely to share cache lines, resulting in the false sharing problem.

Adding padding to each element of the array can solve the issue as long as the padding is large enough to ensure that each element is in a different cache line. However this solution is not portable as different architectures may have different cache line size.

A more portable alternative is to get rid of the array altogether and instead use a single variable shared by all threads. In that case, it is necessary to synchronize the access to that variable to make sure that the updates are atomic.


## How to compile

```bash
#single-threaded version of pi.c
cc pi.c util.c -o out/serialpi
#multi-threaded version of pi.c
/usr/local/opt/llvm/bin/clang -fopenmp pi.c util.c -o out/parpi

#single-threaded version of dandc.c
cc dandc.c util.c -o out/serialdandc
#multi-threaded version of dandc.c
/usr/local/opt/llvm/bin/clang -fopenmp dandc.c util.c -o out/pardandc

/usr/local/opt/llvm/bin/clang -fopenmp spmd.c util.c -o out/spmd
/usr/local/opt/llvm/bin/clang -fopenmp spmd_padded.c util.c -o out/spmd_padded    
/usr/local/opt/llvm/bin/clang -fopenmp spmd_synced.c util.c -o out/spmd_synced
```

## How to run

Number of repetitions, threads and steps can be specified as arguments to the programs, for instance:

```
Usage: ./out/serialpi [-r num_repetitions] [-s num_steps] [-t num_threads]
```

The number of threads is only relevant for the _spmd*_ versions.

If no argument is passed, the default values defined in the header _pi.h_ are used.

## Benchmark

Here's a comparison of the execution time of the different versions based on the following parameters:

- number of steps = 5,000,000,000
- divide and conquer's parallelism factor = 100,000 steps
- padding size of the SPMD version = 64 bytes (= cache line size)
- number of physical cores: 8
- number of logical processors: 16
- worksharing versions (pi.c and dandc.c) use the default number of threads (normally one per logical processor)

The times are measured in seconds and are the average value of 10 runs.

<table>
    <thead>
        <tr>
            <th>threads</th>            
            <th>serial pi</th>
            <th>parallel pi</th>
            <th>serial d&c</th>
            <th>parallel d&c</th>
            <th>spmd</th>
            <th>padded spmd</th>
            <th>synced spmd</th>                                    
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>8</td>
            <td rowspan=4></td>
            <td rowspan=4></td>
            <td rowspan=4></td>
            <td rowspan=4></td>
            <td></td>
            <td></td>
            <td></td>            
        </tr>
        <tr>
            <td>16</td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>100</td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>1000</td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
    </tbody>
</table>


__Analysis__:

- the effect of false sharing is inversely proportional to the number of threads; that makes sense as the array _double sum[num_threads]_ will span more and more cache lines and therefore the probability of 2 threads accessing the same cache line will be lower
- the performance of the _padded_ and _synced_ versions is similar but the _synced_ version is portable across different hardware architectures
